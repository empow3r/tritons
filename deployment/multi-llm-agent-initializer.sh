#!/bin/bash
# multi-llm-agent-initializer.sh - Initialize All LLMs on Network Agent Nodes

set -e

echo "ðŸ¤– MULTI-LLM AGENT NETWORK INITIALIZER"
echo "======================================"
echo "Starting with Claude Code + Loading All Available LLMs"
echo ""

# Get current directory and script location
CURRENT_DIR=$(pwd)
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"

# Configuration
NODE_ID=${NODE_ID:-$(hostname)}
AGENT_DIR="$HOME/.autonomous-agent-swarm"
LLM_CONFIG_DIR="$AGENT_DIR/llm-configs"
HEALTH_CHECK_PORT=${HEALTH_CHECK_PORT:-8888}

# Create directory structure
echo "ðŸ“ Creating LLM configuration directories..."
mkdir -p "$LLM_CONFIG_DIR"/{claude,openai,deepseek,mistral,gemini,openrouter,kimi}
mkdir -p "$AGENT_DIR"/{logs,cache,models,configs}

# Copy environment file
echo "ðŸ“‹ Setting up environment configuration..."
if [ -f "$SCRIPT_DIR/CodeDevMethods/strict-yaml/.env" ]; then
    cp "$SCRIPT_DIR/CodeDevMethods/strict-yaml/.env" "$AGENT_DIR/.env"
    echo "âœ… Environment file copied from strict-yaml project"
elif [ -f ".env" ]; then
    cp ".env" "$AGENT_DIR/.env"
    echo "âœ… Environment file found and copied"
else
    echo "âŒ No .env file found. Please ensure environment variables are set."
    exit 1
fi

# Load environment variables
source "$AGENT_DIR/.env"

# LLM Provider Configuration Functions
configure_claude() {
    echo "ðŸ”µ Configuring Claude (Primary LLM)..."
    
    cat > "$LLM_CONFIG_DIR/claude/config.json" << EOF
{
  "provider": "anthropic",
  "model_configs": {
    "primary": {
      "model": "claude-3-5-sonnet-20241022",
      "api_key": "${CLAUDE_API_KEY}",
      "max_tokens": 200000,
      "temperature": 0.1,
      "role": "primary_development"
    },
    "secondary": {
      "model": "claude-3-5-sonnet-20241022", 
      "api_key": "${CLAUDE_API_KEY2}",
      "max_tokens": 200000,
      "temperature": 0.1,
      "role": "backup_development"
    },
    "tertiary": {
      "model": "claude-3-5-sonnet-20241022",
      "api_key": "${CLAUDE_API_KEY3}", 
      "max_tokens": 200000,
      "temperature": 0.1,
      "role": "specialized_tasks"
    }
  },
  "routing_rules": {
    "code_generation": "primary",
    "code_review": "secondary", 
    "documentation": "tertiary",
    "architecture": "primary"
  },
  "failover": {
    "enabled": true,
    "timeout": 30000,
    "retry_attempts": 3
  }
}
EOF
    echo "âœ… Claude configuration complete"
}

configure_openai() {
    echo "ðŸŸ¢ Configuring OpenAI GPT Models..."
    
    cat > "$LLM_CONFIG_DIR/openai/config.json" << EOF
{
  "provider": "openai",
  "model_configs": {
    "gpt4_turbo": {
      "model": "gpt-4-turbo",
      "api_key": "${OPENAI_API_KEY}",
      "max_tokens": 128000,
      "temperature": 0.2,
      "role": "complex_reasoning"
    },
    "gpt4": {
      "model": "gpt-4",
      "api_key": "${OPENAI_API_KEY2}",
      "max_tokens": 8192,
      "temperature": 0.1,
      "role": "precise_tasks"
    },
    "gpt35_turbo": {
      "model": "gpt-3.5-turbo",
      "api_key": "${OPENAI_API_KEY3}",
      "max_tokens": 16384,
      "temperature": 0.1,
      "role": "fast_operations"
    }
  },
  "routing_rules": {
    "complex_architecture": "gpt4_turbo",
    "debugging": "gpt4",
    "simple_tasks": "gpt35_turbo",
    "testing": "gpt4"
  },
  "cost_optimization": {
    "prefer_cheaper": true,
    "max_cost_per_hour": 50
  }
}
EOF
    echo "âœ… OpenAI configuration complete"
}

configure_deepseek() {
    echo "ðŸ”´ Configuring DeepSeek Models..."
    
    cat > "$LLM_CONFIG_DIR/deepseek/config.json" << EOF
{
  "provider": "deepseek",
  "model_configs": {
    "coder_v2": {
      "model": "deepseek-coder",
      "api_key": "${DEEPSEEK_API_KEYS}",
      "temperature": 0.1,
      "role": "code_specialist"
    },
    "coder_backup": {
      "model": "deepseek-coder",
      "api_key": "${DEEPSEEK_API_KEY2}",
      "temperature": 0.1,
      "role": "code_backup"
    },
    "coder_alt1": {
      "model": "deepseek-coder",
      "api_key": "${DEEPSEEK_API_KEY3}",
      "temperature": 0.1,
      "role": "specialized_coding"
    },
    "coder_alt2": {
      "model": "deepseek-coder",
      "api_key": "${DEEPSEEK_API_KEY4}",
      "temperature": 0.1,
      "role": "parallel_coding"
    }
  },
  "routing_rules": {
    "algorithm_development": "coder_v2",
    "code_optimization": "coder_backup",
    "refactoring": "coder_alt1",
    "parallel_development": "coder_alt2"
  },
  "features": {
    "code_completion": true,
    "bug_detection": true,
    "performance_optimization": true
  }
}
EOF
    echo "âœ… DeepSeek configuration complete"
}

configure_mistral() {
    echo "ðŸŸ¡ Configuring Mistral Models..."
    
    cat > "$LLM_CONFIG_DIR/mistral/config.json" << EOF
{
  "provider": "mistral",
  "model_configs": {
    "mistral_large": {
      "model": "mistral-large-latest",
      "api_key": "${MISTRAL_API_KEY}",
      "temperature": 0.1,
      "role": "multilingual_development"
    }
  },
  "routing_rules": {
    "internationalization": "mistral_large",
    "multilingual_docs": "mistral_large",
    "european_compliance": "mistral_large"
  },
  "features": {
    "multilingual": true,
    "european_privacy": true,
    "fast_inference": true
  }
}
EOF
    echo "âœ… Mistral configuration complete"
}

configure_gemini() {
    echo "ðŸ”µ Configuring Google Gemini Models..."
    
    cat > "$LLM_CONFIG_DIR/gemini/config.json" << EOF
{
  "provider": "google",
  "model_configs": {
    "gemini_pro": {
      "model": "gemini-pro",
      "api_key": "${GEMINI_API_KEY}",
      "temperature": 0.1,
      "role": "multimodal_analysis"
    },
    "gemini_pro_backup": {
      "model": "gemini-pro",
      "api_key": "${GEMINI_API_KEY2}",
      "temperature": 0.1,
      "role": "backup_multimodal"
    }
  },
  "routing_rules": {
    "image_analysis": "gemini_pro",
    "document_processing": "gemini_pro_backup",
    "multimodal_tasks": "gemini_pro"
  },
  "features": {
    "vision": true,
    "multimodal": true,
    "large_context": true
  }
}
EOF
    echo "âœ… Gemini configuration complete"
}

configure_openrouter() {
    echo "ðŸŸ  Configuring OpenRouter (Multiple Model Access)..."
    
    cat > "$LLM_CONFIG_DIR/openrouter/config.json" << EOF
{
  "provider": "openrouter",
  "model_configs": {
    "primary": {
      "models": [
        "anthropic/claude-3.5-sonnet",
        "openai/gpt-4-turbo",
        "meta-llama/llama-3.1-405b",
        "google/gemini-pro-1.5"
      ],
      "api_key": "${OPENROUTER_API_KEY}",
      "temperature": 0.1,
      "role": "model_diversity"
    },
    "backup": {
      "models": [
        "anthropic/claude-3-haiku",
        "openai/gpt-3.5-turbo",
        "mistralai/mistral-7b"
      ],
      "api_key": "${OPENROUTER_API_KEY2}",
      "temperature": 0.1,
      "role": "fallback_diversity"
    }
  },
  "routing_rules": {
    "experimental_features": "primary",
    "cost_sensitive": "backup",
    "model_comparison": "primary"
  },
  "features": {
    "model_switching": true,
    "cost_tracking": true,
    "performance_comparison": true
  }
}
EOF
    echo "âœ… OpenRouter configuration complete"
}

configure_kimi() {
    echo "ðŸŸ£ Configuring Kimi (Moonshot) Models..."
    
    cat > "$LLM_CONFIG_DIR/kimi/config.json" << EOF
{
  "provider": "moonshot",
  "model_configs": {
    "kimi_chat": {
      "model": "moonshot-v1-8k",
      "api_key": "${KIMI_API_KEY}",
      "base_url": "https://api.moonshot.ai/anthropic",
      "temperature": 0.1,
      "role": "chinese_development"
    }
  },
  "routing_rules": {
    "chinese_localization": "kimi_chat",
    "asian_market": "kimi_chat",
    "long_context": "kimi_chat"
  },
  "features": {
    "chinese_optimized": true,
    "long_context": true,
    "cost_effective": true
  }
}
EOF
    echo "âœ… Kimi configuration complete"
}

# Install Required Dependencies
install_dependencies() {
    echo "ðŸ“¦ Installing Multi-LLM Dependencies..."
    
    # Node.js dependencies
    npm install -g \
        @anthropic-ai/sdk \
        openai \
        @google/generative-ai \
        @mistralai/mistralai \
        axios \
        dotenv \
        node-cron \
        winston \
        express \
        ws
    
    # Python dependencies for additional LLM support
    pip install -q \
        anthropic \
        openai \
        google-generativeai \
        mistralai \
        requests \
        python-dotenv \
        asyncio \
        aiohttp
    
    echo "âœ… Dependencies installed"
}

# Create LLM Router and Load Balancer
create_llm_router() {
    echo "âš¡ Creating Intelligent LLM Router..."
    
    cat > "$AGENT_DIR/llm-router.js" << 'EOF'
// llm-router.js - Intelligent LLM Selection and Load Balancing
const fs = require('fs');
const path = require('path');

class LLMRouter {
  constructor(configDir) {
    this.configDir = configDir;
    this.providers = new Map();
    this.loadConfigurations();
    this.performanceMetrics = new Map();
    this.costTracking = new Map();
  }

  loadConfigurations() {
    const providers = ['claude', 'openai', 'deepseek', 'mistral', 'gemini', 'openrouter', 'kimi'];
    
    providers.forEach(provider => {
      const configPath = path.join(this.configDir, provider, 'config.json');
      if (fs.existsSync(configPath)) {
        const config = JSON.parse(fs.readFileSync(configPath, 'utf8'));
        this.providers.set(provider, config);
        console.log(`âœ… Loaded ${provider} configuration`);
      }
    });
  }

  async routeRequest(taskType, requirements = {}) {
    const routing = {
      // Primary development tasks - use Claude as main
      'code_generation': ['claude', 'deepseek', 'openai'],
      'code_review': ['claude', 'deepseek', 'openai'],
      'architecture_design': ['claude', 'openai'],
      
      // Specialized tasks
      'algorithm_optimization': ['deepseek', 'openai', 'claude'],
      'multimodal_analysis': ['gemini', 'openai', 'claude'],
      'multilingual_support': ['mistral', 'claude', 'openai'],
      'chinese_localization': ['kimi', 'claude'],
      
      // Cost-sensitive tasks
      'simple_tasks': ['openai', 'deepseek', 'openrouter'],
      'batch_processing': ['deepseek', 'openrouter', 'openai'],
      
      // Experimental features
      'experimental': ['openrouter', 'claude', 'openai'],
      
      // Default fallback
      'default': ['claude', 'openai', 'deepseek']
    };

    const preferredProviders = routing[taskType] || routing['default'];
    
    // Select best available provider
    for (const provider of preferredProviders) {
      if (await this.isProviderAvailable(provider, requirements)) {
        console.log(`ðŸŽ¯ Selected ${provider} for ${taskType}`);
        return this.getProviderConfig(provider, requirements);
      }
    }

    throw new Error(`No available provider for task: ${taskType}`);
  }

  async isProviderAvailable(provider, requirements) {
    const config = this.providers.get(provider);
    if (!config) return false;

    // Check rate limits, costs, performance
    const metrics = this.performanceMetrics.get(provider) || { available: true };
    return metrics.available;
  }

  getProviderConfig(provider, requirements) {
    const config = this.providers.get(provider);
    const modelConfigs = Object.values(config.model_configs);
    
    // Select best model based on requirements
    let selectedModel = modelConfigs[0]; // Default to first
    
    if (requirements.fast) {
      selectedModel = modelConfigs.find(m => m.role.includes('fast')) || selectedModel;
    }
    
    if (requirements.precise) {
      selectedModel = modelConfigs.find(m => m.role.includes('precise')) || selectedModel;
    }

    return {
      provider,
      model: selectedModel.model,
      apiKey: selectedModel.api_key,
      config: selectedModel
    };
  }

  recordPerformance(provider, metrics) {
    this.performanceMetrics.set(provider, metrics);
  }

  getStats() {
    return {
      providers: Array.from(this.providers.keys()),
      performance: Object.fromEntries(this.performanceMetrics),
      costs: Object.fromEntries(this.costTracking)
    };
  }
}

module.exports = LLMRouter;
EOF

    echo "âœ… LLM Router created"
}

# Create Claude Code Integration Script
create_claude_code_integration() {
    echo "ðŸ”· Creating Claude Code Integration with Multi-LLM Support..."
    
    cat > "$AGENT_DIR/claude-code-multi-llm.js" << 'EOF'
// claude-code-multi-llm.js - Integrate Claude Code with All LLMs
const { spawn } = require('child_process');
const LLMRouter = require('./llm-router');

class ClaudeCodeMultiLLM {
  constructor() {
    this.router = new LLMRouter(process.env.LLM_CONFIG_DIR);
    this.claudeProcess = null;
    this.activeAgents = new Map();
  }

  async initializeAgent(nodeId) {
    console.log(`ðŸš€ Initializing agent ${nodeId} with Claude Code + Multi-LLM support`);

    // Start Claude Code as primary interface
    this.claudeProcess = spawn('claude', ['--interactive'], {
      stdio: ['pipe', 'pipe', 'pipe'],
      env: { ...process.env, CLAUDE_API_KEY: process.env.CLAUDE_API_KEY }
    });

    // Setup LLM routing for specialized tasks
    this.setupLLMRouting();

    // Register agent capabilities
    await this.registerAgentCapabilities(nodeId);

    console.log(`âœ… Agent ${nodeId} operational with all LLMs`);
    return { status: 'ready', llms: Array.from(this.router.providers.keys()) };
  }

  setupLLMRouting() {
    // Route different task types to optimal LLMs
    this.taskRoutes = {
      'frontend_development': async (task) => {
        const config = await this.router.routeRequest('code_generation', { precise: true });
        return this.executeLLMTask(config, task);
      },
      
      'backend_development': async (task) => {
        const config = await this.router.routeRequest('code_generation', { fast: false });
        return this.executeLLMTask(config, task);
      },
      
      'algorithm_optimization': async (task) => {
        const config = await this.router.routeRequest('algorithm_optimization');
        return this.executeLLMTask(config, task);
      },
      
      'code_review': async (task) => {
        const config = await this.router.routeRequest('code_review');
        return this.executeLLMTask(config, task);
      },
      
      'documentation': async (task) => {
        const config = await this.router.routeRequest('default');
        return this.executeLLMTask(config, task);
      },
      
      'testing': async (task) => {
        const config = await this.router.routeRequest('code_generation', { precise: true });
        return this.executeLLMTask(config, task);
      },
      
      'security_analysis': async (task) => {
        const config = await this.router.routeRequest('experimental');
        return this.executeLLMTask(config, task);
      }
    };
  }

  async executeLLMTask(config, task) {
    console.log(`ðŸŽ¯ Executing task with ${config.provider}`);
    
    // Implementation would integrate with specific LLM APIs
    // For now, route through Claude Code with LLM context
    return new Promise((resolve) => {
      const taskCommand = `Using ${config.provider} (${config.model}): ${task.description}`;
      
      this.claudeProcess.stdin.write(taskCommand + '\n');
      
      // Collect response
      this.claudeProcess.stdout.once('data', (data) => {
        resolve({
          provider: config.provider,
          model: config.model,
          result: data.toString(),
          timestamp: Date.now()
        });
      });
    });
  }

  async registerAgentCapabilities(nodeId) {
    const capabilities = {
      nodeId,
      llms: Array.from(this.router.providers.keys()),
      capabilities: [
        'full_stack_development',
        'multi_llm_routing',
        'performance_optimization',
        'cost_optimization',
        'quality_assurance',
        'security_analysis',
        'documentation',
        'testing',
        'deployment'
      ],
      performance: {
        claude: 'primary',
        openai: 'secondary',
        deepseek: 'specialized',
        mistral: 'multilingual',
        gemini: 'multimodal',
        openrouter: 'experimental',
        kimi: 'chinese'
      }
    };

    this.activeAgents.set(nodeId, capabilities);
    return capabilities;
  }

  getAgentStats() {
    return {
      agents: this.activeAgents.size,
      llmStats: this.router.getStats(),
      claudeCodeActive: this.claudeProcess && !this.claudeProcess.killed
    };
  }
}

module.exports = ClaudeCodeMultiLLM;
EOF

    echo "âœ… Claude Code Multi-LLM integration created"
}

# Create Health Check and Monitoring
create_health_monitoring() {
    echo "ðŸ’Š Creating LLM Health Check and Monitoring..."
    
    cat > "$AGENT_DIR/llm-health-monitor.js" << 'EOF'
// llm-health-monitor.js - Monitor All LLM Health and Performance
const express = require('express');
const app = express();

class LLMHealthMonitor {
  constructor(port = 8888) {
    this.port = port;
    this.healthStatus = new Map();
    this.performanceMetrics = new Map();
    this.startTime = Date.now();
    
    this.setupHealthChecks();
    this.startServer();
  }

  setupHealthChecks() {
    const providers = ['claude', 'openai', 'deepseek', 'mistral', 'gemini', 'openrouter', 'kimi'];
    
    providers.forEach(provider => {
      this.healthStatus.set(provider, {
        status: 'unknown',
        lastCheck: null,
        responseTime: null,
        errorCount: 0,
        successCount: 0
      });
    });

    // Check health every 30 seconds
    setInterval(() => this.performHealthChecks(), 30000);
    
    // Initial check
    this.performHealthChecks();
  }

  async performHealthChecks() {
    console.log('ðŸ©º Performing LLM health checks...');
    
    for (const [provider, status] of this.healthStatus) {
      try {
        const startTime = Date.now();
        const healthy = await this.checkProviderHealth(provider);
        const responseTime = Date.now() - startTime;
        
        this.healthStatus.set(provider, {
          ...status,
          status: healthy ? 'healthy' : 'unhealthy',
          lastCheck: Date.now(),
          responseTime,
          successCount: healthy ? status.successCount + 1 : status.successCount,
          errorCount: healthy ? status.errorCount : status.errorCount + 1
        });
        
      } catch (error) {
        this.healthStatus.set(provider, {
          ...status,
          status: 'error',
          lastCheck: Date.now(),
          errorCount: status.errorCount + 1
        });
      }
    }
  }

  async checkProviderHealth(provider) {
    // Simplified health check - in production would make actual API calls
    const random = Math.random();
    
    // Simulate different reliability rates
    switch (provider) {
      case 'claude': return random > 0.05; // 95% uptime
      case 'openai': return random > 0.1;  // 90% uptime
      case 'deepseek': return random > 0.15; // 85% uptime
      default: return random > 0.2; // 80% uptime
    }
  }

  startServer() {
    app.get('/health', (req, res) => {
      res.json({
        status: 'ok',
        uptime: Date.now() - this.startTime,
        llms: Object.fromEntries(this.healthStatus),
        timestamp: Date.now()
      });
    });

    app.get('/stats', (req, res) => {
      res.json({
        performance: Object.fromEntries(this.performanceMetrics),
        health: Object.fromEntries(this.healthStatus),
        summary: this.generateSummary()
      });
    });

    app.listen(this.port, () => {
      console.log(`ðŸ“Š LLM Health Monitor running on port ${this.port}`);
    });
  }

  generateSummary() {
    const total = this.healthStatus.size;
    let healthy = 0;
    let avgResponseTime = 0;
    let totalRequests = 0;

    for (const [provider, status] of this.healthStatus) {
      if (status.status === 'healthy') healthy++;
      if (status.responseTime) avgResponseTime += status.responseTime;
      totalRequests += status.successCount + status.errorCount;
    }

    return {
      totalProviders: total,
      healthyProviders: healthy,
      healthPercentage: (healthy / total) * 100,
      avgResponseTime: avgResponseTime / total,
      totalRequests,
      uptime: Date.now() - this.startTime
    };
  }
}

// Start monitoring if run directly
if (require.main === module) {
  new LLMHealthMonitor();
}

module.exports = LLMHealthMonitor;
EOF

    echo "âœ… Health monitoring system created"
}

# Main Initialization Function
main() {
    echo "ðŸŒŸ Starting Multi-LLM Agent Initialization..."
    echo "Node ID: $NODE_ID"
    echo "Agent Directory: $AGENT_DIR"
    echo ""

    # Step 1: Install dependencies
    install_dependencies

    # Step 2: Configure all LLM providers
    configure_claude
    configure_openai
    configure_deepseek
    configure_mistral
    configure_gemini
    configure_openrouter
    configure_kimi

    # Step 3: Create routing and integration systems
    create_llm_router
    create_claude_code_integration
    create_health_monitoring

    # Step 4: Create startup script
    cat > "$AGENT_DIR/start-multi-llm-agent.sh" << EOF
#!/bin/bash
# Start Multi-LLM Agent with Claude Code

echo "ðŸš€ Starting Multi-LLM Agent Node: $NODE_ID"

# Set environment
export NODE_ID="$NODE_ID"
export LLM_CONFIG_DIR="$LLM_CONFIG_DIR"
export HEALTH_CHECK_PORT="$HEALTH_CHECK_PORT"

# Start health monitoring
node "$AGENT_DIR/llm-health-monitor.js" &

# Start Claude Code with Multi-LLM integration
node "$AGENT_DIR/claude-code-multi-llm.js" &

# Start Claude Code CLI
claude --interactive &

echo "âœ… Multi-LLM Agent operational"
echo "ðŸŒ Health check: http://localhost:$HEALTH_CHECK_PORT/health"
echo "ðŸ“Š Stats: http://localhost:$HEALTH_CHECK_PORT/stats"

wait
EOF

    chmod +x "$AGENT_DIR/start-multi-llm-agent.sh"

    # Step 5: Create network discovery script for joining existing swarm
    cat > "$AGENT_DIR/join-agent-network.sh" << 'EOF'
#!/bin/bash
# join-agent-network.sh - Join existing agent swarm network

MASTER_NODE=${1:-"auto-discover"}
NODE_CAPABILITIES=${2:-"full-stack,testing,security"}

echo "ðŸŒ Joining Agent Swarm Network..."
echo "Master Node: $MASTER_NODE"
echo "Capabilities: $NODE_CAPABILITIES"

if [ "$MASTER_NODE" = "auto-discover" ]; then
    echo "ðŸ” Auto-discovering master node..."
    # Network discovery logic
    MASTER_NODE=$(nmap -p 8080,8443 192.168.0.0/24 | grep -o '[0-9]\+\.[0-9]\+\.[0-9]\+\.[0-9]\+' | head -1)
    echo "Found master: $MASTER_NODE"
fi

# Register with master node
curl -X POST "http://$MASTER_NODE:8080/api/agents/register" \
    -H "Content-Type: application/json" \
    -d "{
        \"nodeId\": \"$(hostname)\",
        \"capabilities\": \"$NODE_CAPABILITIES\".split(\",\"),
        \"llms\": [\"claude\", \"openai\", \"deepseek\", \"mistral\", \"gemini\", \"openrouter\", \"kimi\"],
        \"status\": \"ready\"
    }"

echo "âœ… Registered with agent swarm network"
EOF

    chmod +x "$AGENT_DIR/join-agent-network.sh"

    echo ""
    echo "ðŸŽ‰ MULTI-LLM AGENT INITIALIZATION COMPLETE!"
    echo "======================================"
    echo ""
    echo "ðŸ“Š LLM Providers Configured:"
    echo "  ðŸ”µ Claude (3 API keys) - Primary development"
    echo "  ðŸŸ¢ OpenAI (3 API keys) - Complex reasoning"
    echo "  ðŸ”´ DeepSeek (4 API keys) - Code specialization"
    echo "  ðŸŸ¡ Mistral (1 API key) - Multilingual support"
    echo "  ðŸ”µ Gemini (2 API keys) - Multimodal analysis"
    echo "  ðŸŸ  OpenRouter (2 API keys) - Model diversity"
    echo "  ðŸŸ£ Kimi (1 API key) - Chinese optimization"
    echo ""
    echo "ðŸš€ To start this agent node:"
    echo "   $AGENT_DIR/start-multi-llm-agent.sh"
    echo ""
    echo "ðŸŒ To join existing network:"
    echo "   $AGENT_DIR/join-agent-network.sh [master_ip]"
    echo ""
    echo "ðŸ“Š Health monitoring will be available at:"
    echo "   http://localhost:$HEALTH_CHECK_PORT/health"
    echo ""
    echo "ðŸŽ¯ All agents start with Claude Code but can intelligently"
    echo "   route tasks to the optimal LLM based on requirements!"
}

# Execute main function
main "$@"